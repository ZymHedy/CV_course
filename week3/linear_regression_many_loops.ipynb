{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference 预测y\n",
    "def inference(w,b,x):\n",
    "    pred_y = w * x + b\n",
    "    return pred_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost function\n",
    "def eval_loss(w,b,x_list,gt_y_list):\n",
    "    avg_loss = 0\n",
    "    for i in range(len(x_list)):\n",
    "        avg_loss += 0.5 * (w * x_list[i] + b - gt_y_list[i]) ** 2\n",
    "    avg_loss /= len(gt_y_list)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#单一样本带来的梯度\n",
    "def gradient(pred_y, gt_y, x):\n",
    "    diff = pred_y - gt_y\n",
    "    dw = diff * x\n",
    "    db = diff\n",
    "    return dw,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#全部样本（batchsize）为w,b带来的更新\n",
    "def cal_step_gradient(batch_x_list, batch_gt_y_list, w, b ,lr):\n",
    "    avg_dw, avg_db = 0, 0\n",
    "    batch_size = len(batch_x_list)\n",
    "    for i in range(batch_size):\n",
    "        pred_y = inference(w, b, batch_x_list[i])\n",
    "        dw, db = gradient(pred_y, batch_gt_y_list[i], batch_x_list[i])\n",
    "        avg_dw += dw\n",
    "        avg_db += db\n",
    "    avg_dw /= batch_size\n",
    "    avg_db /= batch_size\n",
    "    w -= lr * avg_dw\n",
    "    b -= lr * avg_db\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sample_data():\n",
    "    w = random.randint(0,10) + random.random()\n",
    "    b = random.randint(0, 5) + random.random()\n",
    "    \n",
    "    num_sample = 100\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    print(w,b)\n",
    "    for i in range(num_sample):\n",
    "        x = random.randint(0,100) * random.random()\n",
    "        y = w * x + b + random.random() * random.randint(-1, 100)\n",
    "        \n",
    "        x_list.append(x)\n",
    "        y_list.append(y)\n",
    "        \n",
    "    return x_list, y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.55479114364241 1.3271190334407807\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_list, y_list = gen_sample_data()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x_list, y_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(x_list, gt_y_list, batch_size, lr, max_iter):\n",
    "    w = 0\n",
    "    b = 0\n",
    "    num_samples = len(x_list)\n",
    "    for i in range(max_iter):\n",
    "        batch_idxs = np.random.choice(len(x_list), batch_size) #随机抽取batch_size个样本的索引值\n",
    "        batch_x = [x_list[j] for j in batch_idxs]\n",
    "        batch_y = [gt_y_list[j] for j in batch_idxs]\n",
    "        w, b = cal_step_gradient(batch_x, batch_y, w, b, lr)\n",
    "        print('w:{0},b:{1}'.format(w,b))\n",
    "        print('loss is {}'.format(eval_loss(w,b,x_list,gt_y_list)))\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:4.795808150847796,b:0.11270900419290789\n",
      "loss is 701.3491984000283\n",
      "w:4.2578215623405935,b:0.10814915845929386\n",
      "loss is 406.77500505822564\n",
      "w:3.972734711567308,b:0.11432678337487605\n",
      "loss is 363.8807218700691\n",
      "w:3.9659246249973776,b:0.12418564979480294\n",
      "loss is 363.6995193047885\n",
      "w:3.8599550675486425,b:0.13167467835479812\n",
      "loss is 368.3344196369074\n",
      "w:3.9444158882973177,b:0.1439786106730469\n",
      "loss is 363.54107843604214\n",
      "w:3.9509156377502372,b:0.15581407546269327\n",
      "loss is 363.32713175626077\n",
      "w:3.9377121738297154,b:0.16593642467025607\n",
      "loss is 363.3822387230064\n",
      "w:3.9633417906211883,b:0.17830807750705915\n",
      "loss is 363.0435440292939\n",
      "w:3.8250382655263593,b:0.18443320401131774\n",
      "loss is 371.46953783048264\n",
      "w:3.989811102746607,b:0.19864794168719369\n",
      "loss is 363.29019038224453\n",
      "w:3.9987946555470066,b:0.2138466592015519\n",
      "loss is 363.4422320523268\n",
      "w:4.009119063086539,b:0.22405755459723445\n",
      "loss is 363.79919785197023\n",
      "w:3.8740848539696624,b:0.2300482688653883\n",
      "loss is 365.7089245219665\n",
      "w:3.9841150912712924,b:0.24543519923568544\n",
      "loss is 362.5985310643426\n",
      "w:3.932456280704045,b:0.2559893120379972\n",
      "loss is 362.3713893196782\n",
      "w:4.0304720192598165,b:0.26951399423643996\n",
      "loss is 364.63140437605426\n",
      "w:4.051831670168541,b:0.28602929414858524\n",
      "loss is 366.23138679971214\n",
      "w:4.0485387255812295,b:0.2988188461524564\n",
      "loss is 365.8053370526053\n",
      "w:3.9602619023132077,b:0.30874057388667686\n",
      "loss is 361.4861226503971\n",
      "w:4.049962194798989,b:0.32320312946538116\n",
      "loss is 365.6981772233441\n",
      "w:3.9513559058492467,b:0.3333261505656182\n",
      "loss is 361.1832715266114\n",
      "w:3.8954735991570395,b:0.3450754232286344\n",
      "loss is 362.69133534630384\n",
      "w:3.92889001128344,b:0.35886772761753\n",
      "loss is 361.1710715136888\n",
      "w:3.885670835506453,b:0.37218230571815103\n",
      "loss is 362.9261373799062\n",
      "w:3.9550299214267475,b:0.3886658138661338\n",
      "loss is 360.52328977635864\n",
      "w:3.90316439355499,b:0.40016833893525866\n",
      "loss is 361.5664881999362\n",
      "w:3.983928438771135,b:0.41401312907233595\n",
      "loss is 360.70256972038374\n",
      "w:4.069932387210424,b:0.42764866367660115\n",
      "loss is 366.77297055943507\n",
      "w:3.9769104248093794,b:0.43589388484286234\n",
      "loss is 360.26524192076323\n",
      "w:3.9428346602163855,b:0.44592279800864576\n",
      "loss is 359.8783606476011\n",
      "w:3.9991123493978664,b:0.45902516600282733\n",
      "loss is 360.79301213562127\n",
      "w:3.9278704165920586,b:0.4694395457368825\n",
      "loss is 359.82087787557487\n",
      "w:3.7872638267587218,b:0.47908378698846743\n",
      "loss is 372.3495625346552\n",
      "w:3.9727718798395832,b:0.4973842929344412\n",
      "loss is 359.4758628696278\n",
      "w:3.986468952668493,b:0.5083272329872478\n",
      "loss is 359.743238145784\n",
      "w:4.004786761290848,b:0.5221740111520792\n",
      "loss is 360.4032568200557\n",
      "w:4.1030535028366195,b:0.5381559782311207\n",
      "loss is 370.1569540886228\n",
      "w:3.898827685443432,b:0.5451645446709159\n",
      "loss is 359.89017034694325\n",
      "w:3.9933919856362188,b:0.5560783020505978\n",
      "loss is 359.4943408378382\n",
      "w:4.085962625849701,b:0.5697158077712011\n",
      "loss is 367.48954815353585\n",
      "w:4.008372712856606,b:0.5783908074325448\n",
      "loss is 360.0138645862132\n",
      "w:3.8922940777430948,b:0.5870250542237136\n",
      "loss is 359.6793877851131\n",
      "w:3.943497032325194,b:0.5995886190638484\n",
      "loss is 358.0345359791387\n",
      "w:3.934106162400374,b:0.6118527784675151\n",
      "loss is 357.9681489690782\n",
      "w:3.868354078167672,b:0.6227413968006074\n",
      "loss is 360.76011138699585\n",
      "w:3.917187151747554,b:0.6346541214220514\n",
      "loss is 358.0414716623851\n",
      "w:3.855542060775398,b:0.6426558838522566\n",
      "loss is 361.53884349348033\n",
      "w:3.966011885572152,b:0.6596570157151185\n",
      "loss is 357.50386343067476\n",
      "w:3.922548070316031,b:0.6708796312938134\n",
      "loss is 357.45470790729456\n",
      "w:3.9408566024703977,b:0.6840351520340496\n",
      "loss is 357.04353741879163\n",
      "w:3.958767322682493,b:0.6996764638978528\n",
      "loss is 356.93373339458833\n",
      "w:3.9448682153022845,b:0.7110359793203151\n",
      "loss is 356.71464909000474\n",
      "w:3.9700101631868776,b:0.7226988053810581\n",
      "loss is 356.8825098512303\n",
      "w:4.055385084518637,b:0.7385143903066463\n",
      "loss is 362.3268343733237\n",
      "w:4.0054338862647905,b:0.748119783953402\n",
      "loss is 358.0853351503167\n",
      "w:3.943475340736088,b:0.7589017076809903\n",
      "loss is 356.15344818663425\n",
      "w:4.001993252440373,b:0.7720536405608269\n",
      "loss is 357.6397774264862\n",
      "w:3.999634395376275,b:0.784911362745045\n",
      "loss is 357.3759442125103\n",
      "w:3.859253696808472,b:0.7927471861124203\n",
      "loss is 359.15811851885223\n",
      "w:3.983381432872256,b:0.8057637688967514\n",
      "loss is 356.39643166089365\n",
      "w:3.842149805366239,b:0.8168847974579968\n",
      "loss is 360.346481019302\n",
      "w:3.8821998714553896,b:0.8271883246545815\n",
      "loss is 357.101601833767\n",
      "w:3.8584299441693046,b:0.8417522628483756\n",
      "loss is 358.55642788032594\n",
      "w:3.9509196552335264,b:0.8574617371444485\n",
      "loss is 355.0423088048004\n",
      "w:3.981765261550412,b:0.8690189372941297\n",
      "loss is 355.65388994231273\n",
      "w:3.8902978333789062,b:0.8799986799551233\n",
      "loss is 355.9832457390274\n",
      "w:3.8998308103189525,b:0.8906311870842927\n",
      "loss is 355.4254940612462\n",
      "w:3.97781898073805,b:0.9018124950634763\n",
      "loss is 355.1531902089501\n",
      "w:3.960671898368272,b:0.9117820824857124\n",
      "loss is 354.5670512689277\n",
      "w:3.926055066663667,b:0.921802482476089\n",
      "loss is 354.34566727949846\n",
      "w:4.012998587542046,b:0.93415099266627\n",
      "loss is 356.6936998541182\n",
      "w:3.9995688950062807,b:0.9452301717294471\n",
      "loss is 355.71816691537816\n",
      "w:4.030616186775812,b:0.9591422226916055\n",
      "loss is 357.8507808085284\n",
      "w:3.9245736997904426,b:0.9672550866595733\n",
      "loss is 353.8221746237193\n",
      "w:3.85585871026477,b:0.9785237441456797\n",
      "loss is 356.9051074369898\n",
      "w:3.8917483837411853,b:0.9936055538627548\n",
      "loss is 354.4643119808114\n",
      "w:4.040360256862767,b:1.0069075862517192\n",
      "loss is 358.31468365683185\n",
      "w:3.8791242759731808,b:1.0144434597825134\n",
      "loss is 354.83881356896046\n",
      "w:4.066597051866106,b:1.0326375110879715\n",
      "loss is 361.01710013068094\n",
      "w:3.901237569412623,b:1.0391509559783778\n",
      "loss is 353.51363194002306\n",
      "w:3.954045696967846,b:1.0526317173950093\n",
      "loss is 352.87224032483\n",
      "w:3.876275996479034,b:1.065232532490619\n",
      "loss is 354.34348045216393\n",
      "w:3.8423634645580473,b:1.0777860580204432\n",
      "loss is 356.7017740944105\n",
      "w:4.011252955210538,b:1.0960153446112246\n",
      "loss is 354.9698781225116\n",
      "w:3.8811718866716176,b:1.1047804197073203\n",
      "loss is 353.56156625741573\n",
      "w:3.9024686446520533,b:1.115090976414205\n",
      "loss is 352.53267371791384\n",
      "w:3.8943579346060844,b:1.1296587155065103\n",
      "loss is 352.640341757759\n",
      "w:4.062489330771184,b:1.1463384312306355\n",
      "loss is 359.53161365014336\n",
      "w:4.025859338551139,b:1.1580081439684529\n",
      "loss is 355.54991448378905\n",
      "w:4.014657771890509,b:1.171787352822572\n",
      "loss is 354.48827579901257\n",
      "w:3.951995960310636,b:1.1826384642665737\n",
      "loss is 351.3880321570548\n",
      "w:3.860206550845275,b:1.1888350532477066\n",
      "loss is 353.76104392577724\n",
      "w:3.8452871895939578,b:1.201854976082768\n",
      "loss is 354.74963309740014\n",
      "w:3.969531488688544,b:1.2152147039041457\n",
      "loss is 351.4963450314277\n",
      "w:3.8753056353384,b:1.2236989432331842\n",
      "loss is 352.3519068975869\n",
      "w:3.9041730619755532,b:1.234059921319578\n",
      "loss is 351.025282902147\n",
      "w:3.877259357072476,b:1.247062402738354\n",
      "loss is 351.94643222151126\n",
      "w:3.949387408747102,b:1.2585953762411857\n",
      "loss is 350.5001702253391\n",
      "w:4.057402839438624,b:1.271040588061114\n",
      "loss is 357.8403074891856\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4.057402839438624, 1.271040588061114)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(x_list, y_list, 100, 0.001, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
