{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference 预测y\n",
    "def inference(w,b,x):\n",
    "    pred_y = w * x + b\n",
    "    return pred_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost function\n",
    "def eval_loss(w,b,x_list,gt_y_list):\n",
    "    avg_loss = 0\n",
    "    for i in range(len(x_list)):\n",
    "        avg_loss += 0.5 * (w * x_list[i] + b - gt_y_list[i]) ** 2\n",
    "    avg_loss /= len(gt_y_list)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#单个样本带来的梯度\n",
    "def gradient(pred_y, gt_y, x):\n",
    "    diff = pred_y - gt_y\n",
    "    dw = diff * x\n",
    "    db = diff\n",
    "    return dw,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#全部样本（batchsize）为w,b带来的更新\n",
    "def cal_step_gradient(batch_x_list, batch_gt_y_list, w, b ,lr):\n",
    "    avg_dw, avg_db = 0, 0\n",
    "    batch_size = len(batch_x_list)\n",
    "    for i in range(batch_size):\n",
    "        pred_y = inference(w, b, batch_x_list[i])\n",
    "        dw, db = gradient(pred_y, batch_gt_y_list[i], batch_x_list[i])\n",
    "        avg_dw += dw\n",
    "        avg_db += db\n",
    "    avg_dw /= batch_size\n",
    "    avg_db /= batch_size\n",
    "    w -= lr * avg_dw\n",
    "    b -= lr * avg_db\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sample_data():\n",
    "    w = random.randint(0,10) + random.random()\n",
    "    b = random.randint(0, 5) + random.random()\n",
    "    \n",
    "    num_sample = 100\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    print(w,b)\n",
    "    for i in range(num_sample):\n",
    "        x = random.randint(0,100) * random.random()\n",
    "        y = w * x + b + random.random() * random.randint(-1, 100)\n",
    "        \n",
    "        x_list.append(x)\n",
    "        y_list.append(y)\n",
    "        \n",
    "    return x_list, y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.9208772876527656 5.821889086985028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_list, y_list = gen_sample_data()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x_list, y_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_list, gt_y_list, batch_size, lr, max_iter):\n",
    "    w = 0\n",
    "    b = 0\n",
    "    num_samples = len(x_list)\n",
    "    for i in range(max_iter):\n",
    "        batch_idxs = np.random.choice(len(x_list), batch_size) #随机抽取batch_size个样本的索引值\n",
    "        batch_x = [x_list[j] for j in batch_idxs]\n",
    "        batch_y = [gt_y_list[j] for j in batch_idxs]\n",
    "        w, b = cal_step_gradient(batch_x, batch_y, w, b, lr)\n",
    "        print('w:{0},b:{1}'.format(w,b))\n",
    "        print('loss is {}'.format(eval_loss(w,b,x_list,gt_y_list)))\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:8.439478174941655,b:0.20328369110644673\n",
      "loss is 417.815037273764\n",
      "w:8.613865344771394,b:0.22277390048215906\n",
      "loss is 411.50922211934346\n",
      "w:8.438220202894867,b:0.2300140634485348\n",
      "loss is 417.54166240168365\n",
      "w:8.627364523281157,b:0.24778717125388944\n",
      "loss is 412.2508487916732\n",
      "w:8.486079987809601,b:0.258861798055052\n",
      "loss is 411.8136424385221\n",
      "w:8.598912465364293,b:0.27323068653921195\n",
      "loss is 409.9823344803814\n",
      "w:8.575515099189776,b:0.2891556427282148\n",
      "loss is 408.87110954305086\n",
      "w:8.417012825713734,b:0.29620263629069427\n",
      "loss is 419.6053154970967\n",
      "w:8.548090263953458,b:0.31295534487093735\n",
      "loss is 408.29700896704986\n",
      "w:8.494552518614062,b:0.3261437432376681\n",
      "loss is 410.14467561445616\n",
      "w:8.585993256755655,b:0.3416599866508296\n",
      "loss is 408.5291975981259\n",
      "w:8.684930756692308,b:0.35637055123074296\n",
      "loss is 418.0176163166468\n",
      "w:8.406596528362117,b:0.36377893076750883\n",
      "loss is 420.17626738705553\n",
      "w:8.676593091184891,b:0.3837763773801978\n",
      "loss is 416.47241260622104\n",
      "w:8.56018586351299,b:0.39596038084916674\n",
      "loss is 407.1903243570746\n",
      "w:8.589289722319466,b:0.4070691611766397\n",
      "loss is 407.8380354692057\n",
      "w:8.565675189044056,b:0.4236759120433729\n",
      "loss is 406.8961125503797\n",
      "w:8.303477979156526,b:0.4307794482566145\n",
      "loss is 442.9022304944063\n",
      "w:8.715120715069933,b:0.45538232173153237\n",
      "loss is 422.353715492466\n",
      "w:8.587260863898843,b:0.4635973584610502\n",
      "loss is 407.0366952981975\n",
      "w:8.630561450013536,b:0.47802589431976245\n",
      "loss is 409.869226960657\n",
      "w:8.707530150648312,b:0.49498349320839435\n",
      "loss is 420.53786039156745\n",
      "w:8.585098262964726,b:0.502059040635511\n",
      "loss is 406.45934499963545\n",
      "w:8.565271895764232,b:0.5131410367732164\n",
      "loss is 405.7116161958446\n",
      "w:8.542503865995613,b:0.523689236880211\n",
      "loss is 405.44183279762785\n",
      "w:8.481080616301337,b:0.535311034864941\n",
      "loss is 407.9652902261139\n",
      "w:8.454721259987648,b:0.5460389774414702\n",
      "loss is 410.3124001749622\n",
      "w:8.658332632839771,b:0.5662333340651982\n",
      "loss is 412.02778249838434\n",
      "w:8.578584694449836,b:0.5780822122519947\n",
      "loss is 405.2414853207218\n",
      "w:8.647969445985709,b:0.5935043362074822\n",
      "loss is 410.4437843851996\n",
      "w:8.495407343084725,b:0.6020564774536693\n",
      "loss is 405.9465057245975\n",
      "w:8.519812702811654,b:0.614937274303318\n",
      "loss is 404.6195834595553\n",
      "w:8.596248657229959,b:0.6295946072581322\n",
      "loss is 405.4394395420899\n",
      "w:8.559435261988773,b:0.6418018008645475\n",
      "loss is 403.9217877080149\n",
      "w:8.37332330431977,b:0.65018523933914\n",
      "loss is 421.36857520015457\n",
      "w:8.453932153302938,b:0.6664019462198971\n",
      "loss is 408.48068362320987\n",
      "w:8.461621562808409,b:0.6798140414289192\n",
      "loss is 407.4700980922451\n",
      "w:8.459374488982906,b:0.692860697752372\n",
      "loss is 407.4907891989709\n",
      "w:8.62448910271307,b:0.7090030951697112\n",
      "loss is 406.66025888694094\n",
      "w:8.445689047260213,b:0.7176211234946546\n",
      "loss is 408.5926840653817\n",
      "w:8.554931045119993,b:0.7347078994764262\n",
      "loss is 402.63648772766186\n",
      "w:8.578475514663515,b:0.7487755858939279\n",
      "loss is 403.0829774640024\n",
      "w:8.34712856034696,b:0.7568327223328878\n",
      "loss is 425.136831218183\n",
      "w:8.675650473476631,b:0.7784875315403074\n",
      "loss is 412.3262337513313\n",
      "w:8.55501629672802,b:0.7868749418769706\n",
      "loss is 401.9520551979854\n",
      "w:8.530091185781892,b:0.800545487607139\n",
      "loss is 401.78175252232774\n",
      "w:8.630738295640228,b:0.8176500145238418\n",
      "loss is 406.05977902947257\n",
      "w:8.656554533488189,b:0.8291243933280426\n",
      "loss is 409.0273134499452\n",
      "w:8.615439236098467,b:0.839161556686425\n",
      "loss is 404.3549920297462\n",
      "w:8.376256598837383,b:0.8444255609692126\n",
      "loss is 417.3080093184551\n",
      "w:8.610457502619585,b:0.8635194979185621\n",
      "loss is 403.65480098061204\n",
      "w:8.578336705544752,b:0.8780928778554722\n",
      "loss is 401.46397999397135\n",
      "w:8.465358415642571,b:0.890265436123039\n",
      "loss is 403.8521768451972\n",
      "w:8.431684184837218,b:0.9048286651998649\n",
      "loss is 407.29091698056675\n",
      "w:8.481357235953645,b:0.9183808989128126\n",
      "loss is 402.1559174887494\n",
      "w:8.480766714685462,b:0.9310849851146729\n",
      "loss is 402.0076081116307\n",
      "w:8.605238673377361,b:0.9491046098812541\n",
      "loss is 402.2459026266887\n",
      "w:8.597494088732152,b:0.9616388073976464\n",
      "loss is 401.5318300422002\n",
      "w:8.550698126245058,b:0.9738383009124921\n",
      "loss is 399.44691285395754\n",
      "w:8.46651525499797,b:0.9828976690322856\n",
      "loss is 402.3336185785099\n",
      "w:8.48903457790523,b:0.9973587233380576\n",
      "loss is 400.4974409804447\n",
      "w:8.54225030927333,b:1.012354680761276\n",
      "loss is 398.86227453801496\n",
      "w:8.550837143192865,b:1.027918820087291\n",
      "loss is 398.74524717003465\n",
      "w:8.467379203072666,b:1.041144372639838\n",
      "loss is 401.37551221403305\n",
      "w:8.526325908002457,b:1.0540590365862275\n",
      "loss is 398.3669422973211\n",
      "w:8.674139554246594,b:1.070509790924411\n",
      "loss is 409.2147213848989\n",
      "w:8.419399584397205,b:1.0741396109319141\n",
      "loss is 406.1718985024885\n",
      "w:8.538522571613719,b:1.0915246047112588\n",
      "loss is 397.8008706801547\n",
      "w:8.663229136756161,b:1.10642877003418\n",
      "loss is 407.15659648205127\n",
      "w:8.653777560633243,b:1.1199759270432526\n",
      "loss is 405.6506784752336\n",
      "w:8.46405184813434,b:1.127536524160496\n",
      "loss is 400.3443332279361\n",
      "w:8.581470435937003,b:1.1407168116511326\n",
      "loss is 398.404165918238\n",
      "w:8.404153841003586,b:1.1504304418951259\n",
      "loss is 407.14663736908756\n",
      "w:8.52061260888415,b:1.1677570987271262\n",
      "loss is 396.9029277194546\n",
      "w:8.438369593260376,b:1.1801169864489223\n",
      "loss is 402.0745349354586\n",
      "w:8.504253799587637,b:1.1977726519833678\n",
      "loss is 396.9117414459054\n",
      "w:8.413827696624377,b:1.2099553605733142\n",
      "loss is 404.7270938867512\n",
      "w:8.637788814935316,b:1.2267019204084124\n",
      "loss is 402.444985317596\n",
      "w:8.678724242230926,b:1.2393369736106514\n",
      "loss is 408.3671932789753\n",
      "w:8.458958812454535,b:1.2492298989300699\n",
      "loss is 398.9364484439365\n",
      "w:8.51890164288344,b:1.2639381449256857\n",
      "loss is 395.6218793023384\n",
      "w:8.461663614253135,b:1.2777774031059206\n",
      "loss is 398.27369375096794\n",
      "w:8.55551185872512,b:1.2946322350911332\n",
      "loss is 395.43566605771827\n",
      "w:8.403348553472117,b:1.3050416373435583\n",
      "loss is 404.6967355847703\n",
      "w:8.55966393129264,b:1.3242584985801265\n",
      "loss is 395.1923661619083\n",
      "w:8.558713576544259,b:1.337931965981632\n",
      "loss is 394.9904047864698\n",
      "w:8.610073111006441,b:1.35380370923036\n",
      "loss is 398.05989094069474\n",
      "w:8.59634112114461,b:1.3675093348279233\n",
      "loss is 396.7239676665568\n",
      "w:8.61033799209081,b:1.3802054462460893\n",
      "loss is 397.79222484126024\n",
      "w:8.479041915576408,b:1.3848664934899826\n",
      "loss is 395.44251147396494\n",
      "w:8.56302957394509,b:1.3990245447513243\n",
      "loss is 394.3903587739159\n",
      "w:8.554868672286334,b:1.4134073236131481\n",
      "loss is 393.92717346576575\n",
      "w:8.38811611850089,b:1.4217451738031652\n",
      "loss is 405.16939632372925\n",
      "w:8.567830041126228,b:1.4391998492850497\n",
      "loss is 394.10587204677825\n",
      "w:8.650586550451226,b:1.454927550485692\n",
      "loss is 401.8266540941004\n",
      "w:8.489115916881488,b:1.4642942860416666\n",
      "loss is 393.76878389336395\n",
      "w:8.690959933076533,b:1.4817782013052325\n",
      "loss is 408.3860201060974\n",
      "w:8.547650175272693,b:1.4909554373903426\n",
      "loss is 392.75896485490046\n",
      "w:8.443133097575672,b:1.4990699250948294\n",
      "loss is 396.60396618044877\n",
      "w:8.714569446775501,b:1.5183248788242265\n",
      "loss is 412.97015780333555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8.714569446775501, 1.5183248788242265)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(x_list, y_list, 100, 0.001, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
